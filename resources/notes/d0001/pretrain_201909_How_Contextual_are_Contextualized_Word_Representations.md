# How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings
[toc]

### Abstract
- 预训练模型得到的表示向量的语境化程度到底有多高，它们只是简单的为每个词在不同的上下文提供一个单独的表示，还是为每个词只分配有限量的表示向量？

### 1 Introduction

### 2 Related Work

### 3 Approach

### 4 Findings

### 5 Future Work

### 6 Conclusion